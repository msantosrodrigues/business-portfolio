{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ee8ed0",
   "metadata": {},
   "source": [
    "### Part 1: setting up the environment\n",
    "\n",
    "Here I am gonna set the brilliant basics of our environment. \n",
    "I skipped the dependency install because most of us, data analyst/scientist, already have those on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c9f76-670b-4846-a601-d911b896472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import re\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"All dependencies are running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d598c0",
   "metadata": {},
   "source": [
    "### Part 2: data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset loading\n",
    "data = pd.read_csv('test.csv')\n",
    "print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333b386",
   "metadata": {},
   "source": [
    "Let's evaluate some issues our data might have, as missing or invalid values in the rows. Here we want to know what columns and how many rows we have NULL values. \n",
    "\n",
    "This is going to help us to decide whether to act because not every missing row should be filled due some columns not being important here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07526e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentage = (data.isnull().sum() / len(data)) * 100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453322ee",
   "metadata": {},
   "source": [
    "Look closely: the metered fare column has only 0.40% of missing values, probably showing failed trips because... well, a finished trip always have an actual fare. As we knew, the upfront price has 31% of missing values, maybe of rides started without waiting for the prediction. \n",
    "\n",
    "Those missing values are going to be filled later, when we have the prediction model.\n",
    "\n",
    "As we see, there's no need to fill missing values right now. Instead, we are going to remove those from those two columns, so they won't get in our way in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646861c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['metered_price', 'upfront_price'])\n",
    "\n",
    "print(f\"Remaining rows: {len(data)}\")\n",
    "print(f\"% of NaN in metered_price: {data['metered_price'].isnull().mean() * 100:.2f}%\")\n",
    "print(f\"% of NaN in upfront_price: {data['upfront_price'].isnull().mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f6a66b",
   "metadata": {},
   "source": [
    "Now let's remove duplicates, using the column 'order_id_new' since it's the unique ID of each trip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55889af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset='order_id_new')\n",
    "\n",
    "print(f\"Remaining rows: {len(data)}\")\n",
    "print(f\"% of NaN in order_id_new: {data['order_id_new'].isnull().mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6bb05",
   "metadata": {},
   "source": [
    "As we been told in case explanation, every time the metered fare goes 20% above or below the upfront fare, we run on the metered fare. \n",
    "\n",
    "It's important to flag when this happen, so let's create a column beside the upfront fare to see it. \n",
    "- True (1) for when it happens; \n",
    "- and False (0) when it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aafc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage deviation\n",
    "data['deviation_percent'] = abs(data['metered_price'] - data['upfront_price']) / data['upfront_price'] * 100\n",
    "\n",
    "# Create a flag column for 20% deviation\n",
    "data['deviation_flag'] = (data['deviation_percent'] > 20).astype(int)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(data[['metered_price', 'upfront_price', 'deviation_percent', 'deviation_flag']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0106e",
   "metadata": {},
   "source": [
    "After removing the NaN, nulls, missings, etc., we got to take care of the outliers. \n",
    "\n",
    "But, before, we create a backup copy of dataset named filtered_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9308d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da830850",
   "metadata": {},
   "source": [
    "Dealing with outliers using interquartile range separation and quartile splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, columns):\n",
    "    \"\"\"\n",
    "    Removes outliers from specified columns in a DataFrame using the IQR method.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame to process.\n",
    "        columns (list): The list of columns to apply IQR outlier removal.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame without outliers in the specified columns.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Filter data within IQR bounds\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define columns to check for outliers\n",
    "columns_to_check = ['distance', 'duration', 'metered_price', 'upfront_price']\n",
    "\n",
    "# Remove outliers from the filtered_data DataFrame\n",
    "filtered_data_no_outliers = remove_outliers_iqr(filtered_data.copy(), columns_to_check)\n",
    "\n",
    "print(f\"Rows before: {filtered_data.shape[0]}\")\n",
    "print(f\"Rows after: {filtered_data_no_outliers.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788692a2",
   "metadata": {},
   "source": [
    "Splitting in long and short rides..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19913f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q3 (75th percentile) as the threshold for splitting rides\n",
    "Q3 = filtered_data_no_outliers['metered_price'].quantile(0.75)\n",
    "\n",
    "# Split into short and long rides\n",
    "short_rides = filtered_data_no_outliers[filtered_data_no_outliers['metered_price'] <= Q3].copy()\n",
    "long_rides = filtered_data_no_outliers[filtered_data_no_outliers['metered_price'] > Q3].copy()\n",
    "\n",
    "print(f\"Threshold for short rides (Q3): {Q3:.2f}\")\n",
    "print(f\"Short rides: {short_rides.shape[0]} rows\")\n",
    "print(f\"Long rides: {long_rides.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f4aa5",
   "metadata": {},
   "source": [
    "I think there's no need for transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc513f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_data_no_outliers['distance_log'] = np.log1p(filtered_data_no_outliers['distance'])\n",
    "# filtered_data_no_outliers['duration_log'] = np.log1p(filtered_data_no_outliers['duration'])\n",
    "# filtered_data_no_outliers['metered_price_log'] = np.log1p(filtered_data_no_outliers['metered_price'])\n",
    "# filtered_data_no_outliers['upfront_price_log'] = np.log1p(filtered_data_no_outliers['upfront_price'])\n",
    "\n",
    "# # Log-transform distance, duration, and prices for short rides\n",
    "# short_rides['distance_log'] = np.log1p(short_rides['distance'])\n",
    "# short_rides['duration_log'] = np.log1p(short_rides['duration'])\n",
    "# short_rides['metered_price_log'] = np.log1p(short_rides['metered_price'])\n",
    "# short_rides['upfront_price_log'] = np.log1p(short_rides['upfront_price'])\n",
    "\n",
    "# # Log-transform distance, duration, and prices for long rides\n",
    "# long_rides['distance_log'] = np.log1p(long_rides['distance'])\n",
    "# long_rides['duration_log'] = np.log1p(long_rides['duration'])\n",
    "# long_rides['metered_price_log'] = np.log1p(long_rides['metered_price'])\n",
    "# long_rides['upfront_price_log'] = np.log1p(long_rides['upfront_price'])\n",
    "\n",
    "# print(\"Log transformations applied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate statistics\n",
    "# def calculate_statistics(df, columns):\n",
    "#     stats = {}\n",
    "#     for col in columns:\n",
    "#         stats[col] = {\n",
    "#             'mean': df[col].mean(),\n",
    "#             'std': df[col].std(),\n",
    "#             'skew': df[col].skew(),\n",
    "#             'kurtosis': df[col].kurtosis()\n",
    "#         }\n",
    "#     return pd.DataFrame(stats).T\n",
    "\n",
    "# # Columns before and after transformations\n",
    "# original_columns = ['distance', 'duration', 'metered_price', 'upfront_price']\n",
    "# log_transformed_columns = ['distance_log', 'duration_log', 'metered_price_log', 'upfront_price_log']\n",
    "\n",
    "# # Calculate statistics before and after transformations\n",
    "# stats_before = calculate_statistics(filtered_data_no_outliers, original_columns)\n",
    "# stats_after = calculate_statistics(filtered_data_no_outliers, log_transformed_columns)\n",
    "\n",
    "# # Display the statistics\n",
    "# print(\"Statistics Before Transformations:\")\n",
    "# print(stats_before)\n",
    "\n",
    "# print(\"\\nStatistics After Transformations:\")\n",
    "# print(stats_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471be524",
   "metadata": {},
   "source": [
    "Plotting before and after the transformations to analyse the changes of skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Create subplots for each variable before and after transformation\n",
    "# fig, axes = plt.subplots(4, 2, figsize=(14, 16))\n",
    "# fig.suptitle('Distributions Before and After Log Transformations', fontsize=16)\n",
    "\n",
    "# # List of variables and their log-transformed counterparts\n",
    "# variables = ['distance', 'duration', 'metered_price', 'upfront_price']\n",
    "# log_variables = ['distance_log', 'duration_log', 'metered_price_log', 'upfront_price_log']\n",
    "\n",
    "# # Plot each variable before and after transformation\n",
    "# for i, var in enumerate(variables):\n",
    "#     # Plot original variable\n",
    "#     sns.histplot(filtered_data_no_outliers[var], kde=True, ax=axes[i, 0], bins=30)\n",
    "#     axes[i, 0].set_title(f'{var.capitalize()} (Before Transformation)')\n",
    "#     axes[i, 0].set_xlabel(var.capitalize())\n",
    "#     axes[i, 0].set_ylabel('Frequency')\n",
    "    \n",
    "#     # Plot log-transformed variable\n",
    "#     sns.histplot(filtered_data_no_outliers[log_variables[i]], kde=True, ax=axes[i, 1], bins=30)\n",
    "#     axes[i, 1].set_title(f'{var.capitalize()} (After Log Transformation)')\n",
    "#     axes[i, 1].set_xlabel(f'{var.capitalize()} (Log)')\n",
    "#     axes[i, 1].set_ylabel('Frequency')\n",
    "\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef4f16",
   "metadata": {},
   "source": [
    "### Part 3: data exploration and model creation\n",
    "\n",
    "Now that we cleaned and split the data into short and long rides, we aim to model the fare calculation. Fares are often calculated using linear combinations of distance, time, and base fare, real-world data may include non-lineartiy and interactions (as dynamic fare multipliers). Because of this, we are going to use Random Forest, a  non-linear model, to capture these relationships without assuming a pre-defined function. Specifically, we want to predict the upfront prices based on:\t\n",
    "\n",
    "- Distance.\n",
    "\n",
    "- Duration.\n",
    "\n",
    "- Base fare\n",
    "\n",
    "The model allows us to approximate the fare calculation while dealing with complexities and outliers in the data.\n",
    "\n",
    "Using a regression algorithm, we are going to learn and discover how the pricing equation behaves, separated by short and long, due their differences. Due the non-linear relation, let's use something different than the usual linear regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short Rides Random Forest Model\n",
    "print(\"=== Short Rides ===\")\n",
    "\n",
    "# Define features and target for short rides\n",
    "X_short = short_rides[['distance', 'duration', 'upfront_price']]\n",
    "y_short = short_rides['metered_price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_short, X_test_short, y_train_short, y_test_short = train_test_split(\n",
    "    X_short, y_short, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model_short = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_short.fit(X_train_short, y_train_short)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_short = rf_model_short.predict(X_test_short)\n",
    "mae_short = mean_absolute_error(y_test_short, y_pred_short)\n",
    "mse_short = mean_squared_error(y_test_short, y_pred_short)\n",
    "r2_short = r2_score(y_test_short, y_pred_short)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae_short}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_short}\")\n",
    "print(f\"R-squared (R2): {r2_short}\")\n",
    "\n",
    "# Long Rides Random Forest Model\n",
    "print(\"\\n=== Long Rides ===\")\n",
    "\n",
    "# Define features and target for long rides\n",
    "X_long = long_rides[['distance', 'duration', 'upfront_price']]\n",
    "y_long = long_rides['metered_price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_long, X_test_long, y_train_long, y_test_long = train_test_split(\n",
    "    X_long, y_long, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model_long = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_long.fit(X_train_long, y_train_long)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_long = rf_model_long.predict(X_test_long)\n",
    "mae_long = mean_absolute_error(y_test_long, y_pred_long)\n",
    "mse_long = mean_squared_error(y_test_long, y_pred_long)\n",
    "r2_long = r2_score(y_test_long, y_pred_long)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae_long}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_long}\")\n",
    "print(f\"R-squared (R2): {r2_long}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49074c79",
   "metadata": {},
   "source": [
    "Now we have a fairly assertive model to understand how metered prices work using basic variables as distance and time. Let's bring the upfront prices, repeating the analysis to understand in the same variables where they came from and compare with the metered prices, to understand the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a708e",
   "metadata": {},
   "source": [
    "There's some spread here because we aren't taking in consideration some external variables as GPS and destination changes, but the R-squared gives some degree of trustworthiness in the model. By now, that's enough. \n",
    "\n",
    "Now, let's keep looking to the upfront prices and understand how much some variables influenced them, mainly in the bad previsions, and that's why we kept them before.\n",
    "\n",
    "- Rider/driver app version: some versions may be buggy about predicting pricing.\n",
    "- Device brand/model: some devices may only run buggy versions.\n",
    "- GPS confidence: this one is special. We can cross it with device specifications to see what models are likely to deliver bad predictions.\n",
    "\n",
    "To understand the weight of each column in the prediction, we are going to use the Information Value (IV) tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rider app version treatment\n",
    "filtered_data_no_outliers['rider_app_version'] = (\n",
    "    filtered_data_no_outliers['rider_app_version']\n",
    "    .fillna('0')\n",
    "    .str.replace(r'[^0-9.]', '', regex=True)\n",
    "    .str.lstrip('.')  \n",
    "    .replace('', '0')  \n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "#Rider app version treatment\n",
    "filtered_data_no_outliers['driver_app_version'] = (\n",
    "    filtered_data_no_outliers['driver_app_version']\n",
    "    .fillna('0')\n",
    "    .str.replace(r'[^0-9.]', '', regex=True)\n",
    "    .str.lstrip('.') \n",
    "    .replace('', '0')\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "print(filtered_data_no_outliers[['rider_app_version', 'driver_app_version']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe2a9d",
   "metadata": {},
   "source": [
    "Data treatment in the device name, generating the device brand column to identify the iOS vs. Android. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c03763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_brand(device_name):\n",
    "    # Substituir \"iPhone\" por \"iPhone\"\n",
    "    if 'iphone' in device_name.lower():\n",
    "        return 'iPhone'\n",
    "    \n",
    "    # Remover separadores e capturar a primeira palavra\n",
    "    cleaned_name = re.split(r'[ _\\-,]', device_name)\n",
    "    return cleaned_name[0].strip()\n",
    "\n",
    "# Aplicar a função para a coluna 'device_name'\n",
    "filtered_data_no_outliers['device_brand'] = filtered_data_no_outliers['device_name'].apply(extract_brand)\n",
    "\n",
    "# Exibir as primeiras linhas para validação\n",
    "print(filtered_data_no_outliers[['device_name', 'device_brand']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8614b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information value and weight of evidence\n",
    "def calculate_woe_iv(df, feature, target):\n",
    "    grouped = df.groupby(feature)[target].agg(['count', 'sum'])\n",
    "    grouped.columns = ['total', 'event']\n",
    "    grouped['non_event'] = grouped['total'] - grouped['event']\n",
    "    \n",
    "    grouped['event_rate'] = grouped['event'] / grouped['event'].sum()\n",
    "    grouped['non_event_rate'] = grouped['non_event'] / grouped['non_event'].sum()\n",
    "    \n",
    "    grouped['WoE'] = np.log((grouped['event_rate'] + 1e-10) / (grouped['non_event_rate'] + 1e-10))\n",
    "    grouped['IV'] = (grouped['event_rate'] - grouped['non_event_rate']) * grouped['WoE']\n",
    "    \n",
    "    iv = grouped['IV'].sum()\n",
    "    return grouped[['event_rate', 'non_event_rate', 'WoE', 'IV']], iv\n",
    "\n",
    "woe_iv_rider_app, iv_rider_app = calculate_woe_iv(filtered_data_no_outliers, 'rider_app_version', 'deviation_flag')\n",
    "\n",
    "woe_iv_driver_app, iv_driver_app = calculate_woe_iv(filtered_data_no_outliers, 'driver_app_version', 'deviation_flag')\n",
    "\n",
    "woe_iv_device_name, iv_device_brand = calculate_woe_iv(filtered_data_no_outliers, 'device_brand', 'deviation_flag')\n",
    "\n",
    "woe_iv_device_name, iv_device_name = calculate_woe_iv(filtered_data_no_outliers, 'device_name', 'deviation_flag')\n",
    "\n",
    "woe_iv_gps_confidence, iv_gps_confidence = calculate_woe_iv(filtered_data_no_outliers, 'fraud_score', 'deviation_flag')\n",
    "\n",
    "\n",
    "print(\"\\nRider App Version\")\n",
    "print(f\"Information Value: {iv_rider_app:.4f}\")\n",
    "\n",
    "print(\"\\nDriver App Version\")\n",
    "print(f\"Information Value: {iv_driver_app:.4f}\")\n",
    "\n",
    "print(\"\\nDevice brand\")\n",
    "print(f\"Information Value: {iv_device_brand:.4f}\")\n",
    "\n",
    "print(\"\\nDevice name\")\n",
    "print(f\"Information Value: {iv_device_name:.4f}\")\n",
    "\n",
    "print(\"\\nGPS confidence\")\n",
    "print(f\"Information Value: {iv_gps_confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbdf2b3",
   "metadata": {},
   "source": [
    "Now we have the weight of influence of each column in the prediction.\n",
    "\n",
    "To avoid bloating the dataset with boolean columns to mark devices, we are going to use a feature interaction to cross both variables (device name and rider app version) and create a new one, more useful to our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d97e76a",
   "metadata": {},
   "source": [
    "### Part 4: the end\n",
    "\n",
    "Long road, isn't? Almost wrapping everything up.\n",
    "Now we are going to use the model to really predict the new upfront prices, based on what we did above. The last output is a new column, with predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Função para aplicar Target Encoding com a mediana\n",
    "def target_encoding_with_median(dataframe, column, target):\n",
    "    \"\"\"\n",
    "    Aplica target encoding usando a mediana do target em relação a uma coluna categórica.\n",
    "    Adiciona uma nova coluna com os valores da mediana e substitui nulos pela mediana geral.\n",
    "    \"\"\"\n",
    "    median_values = dataframe.groupby(column)[target].median()\n",
    "    new_column_name = f\"{column}_median_target\"\n",
    "    dataframe[new_column_name] = dataframe[column].map(median_values)\n",
    "    dataframe[new_column_name] = dataframe[new_column_name].fillna(dataframe[new_column_name].median())\n",
    "    return dataframe\n",
    "\n",
    "# Aplicar Target Encoding com mediana para 'device_name' e 'rider_app_version'\n",
    "filtered_data_no_outliers = target_encoding_with_median(filtered_data_no_outliers, 'device_name', 'metered_price')\n",
    "filtered_data_no_outliers = target_encoding_with_median(filtered_data_no_outliers, 'rider_app_version', 'metered_price')\n",
    "\n",
    "# Nenhuma transformação na variável categórica 'gps_confidence'\n",
    "filtered_data_no_outliers['gps_confidence_clean'] = filtered_data_no_outliers['gps_confidence']\n",
    "\n",
    "# Verificar se as novas colunas foram criadas\n",
    "print(\"Visualização das colunas criadas:\")\n",
    "print(filtered_data_no_outliers[['device_name_median_target', 'rider_app_version_median_target', 'gps_confidence_clean']].head())\n",
    "\n",
    "# Filtragem para Corridas Curtas e Longas\n",
    "quartiles = filtered_data_no_outliers['metered_price'].quantile([0.25, 0.5, 0.75])\n",
    "Q1, median, Q3 = quartiles[0.25], quartiles[0.5], quartiles[0.75]\n",
    "threshold = (median + Q3) / 2\n",
    "\n",
    "short_rides = filtered_data_no_outliers[filtered_data_no_outliers['metered_price'] <= threshold]\n",
    "long_rides = filtered_data_no_outliers[filtered_data_no_outliers['metered_price'] > threshold]\n",
    "\n",
    "# Recursos e Target para o modelo\n",
    "features = [\n",
    "    'distance', \n",
    "    'duration', \n",
    "    'device_name_median_target', \n",
    "    'rider_app_version_median_target',\n",
    "    'gps_confidence_clean'  # Variável categórica sem transformação\n",
    "]\n",
    "target = 'metered_price'\n",
    "\n",
    "# Preparação dos dados para o modelo\n",
    "X_short = short_rides[features]\n",
    "y_short = short_rides[target]\n",
    "\n",
    "X_long = long_rides[features]\n",
    "y_long = long_rides[target]\n",
    "\n",
    "# Validação das preparações\n",
    "print(\"\\nDados preparados para o modelo:\")\n",
    "print(f\"Corridas Curtas: X_short = {X_short.shape}, y_short = {y_short.shape}\")\n",
    "print(f\"Corridas Longas: X_long = {X_long.shape}, y_long = {y_long.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3df368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Function for target encoding using the median\n",
    "def target_encoding_with_median(dataframe, column, target):\n",
    "    \"\"\"\n",
    "    Applies target encoding using the median of the target relative to a categorical column.\n",
    "    Adds a new column with median values and replaces nulls with the overall median.\n",
    "    \"\"\"\n",
    "    median_values = dataframe.groupby(column)[target].median()\n",
    "    new_column_name = f\"{column}_median_target\"\n",
    "    dataframe[new_column_name] = dataframe[column].map(median_values)\n",
    "    dataframe[new_column_name] = dataframe[new_column_name].fillna(dataframe[new_column_name].median())\n",
    "    return dataframe\n",
    "\n",
    "# Apply target encoding for 'device_name' and 'rider_app_version'\n",
    "filtered_data_no_outliers = target_encoding_with_median(filtered_data_no_outliers, 'device_name', 'metered_price')\n",
    "filtered_data_no_outliers = target_encoding_with_median(filtered_data_no_outliers, 'rider_app_version', 'metered_price')\n",
    "\n",
    "# Retain 'gps_confidence' without transformation\n",
    "filtered_data_no_outliers['gps_confidence_clean'] = filtered_data_no_outliers['gps_confidence']\n",
    "\n",
    "# Define threshold for splitting rides into short and long\n",
    "quartiles = filtered_data_no_outliers['metered_price'].quantile([0.25, 0.5, 0.75])\n",
    "Q1, median, Q3 = quartiles[0.25], quartiles[0.5], quartiles[0.75]\n",
    "threshold = Q3\n",
    "\n",
    "short_rides = filtered_data_no_outliers[filtered_data_no_outliers['metered_price'] <= threshold]\n",
    "long_rides = filtered_data_no_outliers[filtered_data_no_outliers['metered_price'] > threshold]\n",
    "\n",
    "# Features and target for the model\n",
    "features = [\n",
    "    'distance', \n",
    "    'duration', \n",
    "    'device_name_median_target', \n",
    "    'rider_app_version_median_target',\n",
    "    'gps_confidence_clean'\n",
    "]\n",
    "target = 'metered_price'\n",
    "\n",
    "# Prepare data for the model\n",
    "X_short = short_rides[features]\n",
    "y_short = short_rides[target]\n",
    "\n",
    "X_long = long_rides[features]\n",
    "y_long = long_rides[target]\n",
    "\n",
    "# Random Forest Model for Short Rides\n",
    "print(\"\\n=== Short Rides ===\")\n",
    "X_train_short, X_test_short, y_train_short, y_test_short = train_test_split(X_short, y_short, test_size=0.2, random_state=42)\n",
    "rf_model_short = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_short.fit(X_train_short, y_train_short)\n",
    "y_pred_short = rf_model_short.predict(X_test_short)\n",
    "\n",
    "# Evaluate Short Rides Model\n",
    "mae_short = mean_absolute_error(y_test_short, y_pred_short)\n",
    "mse_short = mean_squared_error(y_test_short, y_pred_short)\n",
    "r2_short = r2_score(y_test_short, y_pred_short)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_short}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_short}\")\n",
    "print(f\"R-squared (R2): {r2_short}\")\n",
    "\n",
    "# Random Forest Model for Long Rides\n",
    "print(\"\\n=== Long Rides ===\")\n",
    "X_train_long, X_test_long, y_train_long, y_test_long = train_test_split(X_long, y_long, test_size=0.2, random_state=42)\n",
    "rf_model_long = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_long.fit(X_train_long, y_train_long)\n",
    "y_pred_long = rf_model_long.predict(X_test_long)\n",
    "\n",
    "# Evaluate Long Rides Model\n",
    "mae_long = mean_absolute_error(y_test_long, y_pred_long)\n",
    "mse_long = mean_squared_error(y_test_long, y_pred_long)\n",
    "r2_long = r2_score(y_test_long, y_pred_long)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_long}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_long}\")\n",
    "print(f\"R-squared (R2): {r2_long}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Função para plotar gráficos\n",
    "def plot_results(y_test, y_pred, title_prefix):\n",
    "    # Garantir que y_test e y_pred possuem o mesmo tamanho\n",
    "    y_test = y_test[:len(y_pred)]  # Ajustar o tamanho de y_test\n",
    "    \n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Gráfico: Valores Reais vs Preditos\n",
    "    axes[0].scatter(y_test, y_pred, color='blue', alpha=0.6)\n",
    "    axes[0].plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', lw=2)\n",
    "    axes[0].set_title(f'{title_prefix}: Reais vs Preditos')\n",
    "    axes[0].set_xlabel('Valores Reais')\n",
    "    axes[0].set_ylabel('Valores Preditos')\n",
    "    \n",
    "    # Gráfico: Resíduos vs Preditos\n",
    "    axes[1].scatter(y_pred, residuals, color='orange', alpha=0.6)\n",
    "    axes[1].axhline(0, color='red', linestyle='--', lw=2)\n",
    "    axes[1].set_title(f'{title_prefix}: Resíduos vs Preditos')\n",
    "    axes[1].set_xlabel('Valores Preditos')\n",
    "    axes[1].set_ylabel('Resíduos')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Garantir a consistência entre valores para evitar erros\n",
    "y_test_short = y_test_short[:len(y_pred_short)]\n",
    "y_test_long = y_test_long[:len(y_pred_long)]\n",
    "\n",
    "# Plotar resultados para Corridas Curtas\n",
    "plot_results(y_test_short, y_pred_short, 'Corridas Curtas')\n",
    "\n",
    "# Plotar resultados para Corridas Longas\n",
    "plot_results(y_test_long, y_pred_long, 'Corridas Longas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
