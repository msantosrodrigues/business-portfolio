{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqfsu0D6HRtJ"
   },
   "source": [
    "Importing the libraries we need to start up the things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8-jLh8fHNz2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO4Vm4mFHFRx"
   },
   "source": [
    "### 0. SQL QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZ6mKyeMHIsW"
   },
   "outputs": [],
   "source": [
    "query1 = f\"\"\"\n",
    "WITH MonthlyPurchases AS (\n",
    "    SELECT DISTINCT\n",
    "        product_id,\n",
    "        DATE_TRUNC('month', CAST(purchase_release_datetime AS TIMESTAMP)) AS purchase_month\n",
    "    FROM '{csv_path}'\n",
    "    WHERE product_id IS NOT NULL\n",
    "),\n",
    "\n",
    "LaggedMonths AS (\n",
    "    SELECT\n",
    "        product_id,\n",
    "        purchase_month,\n",
    "        LAG(purchase_month, 1) OVER (PARTITION BY product_id ORDER BY purchase_month) AS previous_month\n",
    "    FROM MonthlyPurchases\n",
    "),\n",
    "\n",
    "StreakIdentifier AS (\n",
    "    SELECT\n",
    "        product_id,\n",
    "        purchase_month,\n",
    "        CASE\n",
    "            WHEN purchase_month - INTERVAL 1 MONTH = previous_month THEN 0\n",
    "            ELSE 1\n",
    "        END AS is_new_streak\n",
    "    FROM LaggedMonths\n",
    "),\n",
    "StreakGroups AS (\n",
    "    SELECT\n",
    "        product_id,\n",
    "        purchase_month,\n",
    "        SUM(is_new_streak) OVER (PARTITION BY product_id ORDER BY purchase_month) AS streak_group\n",
    "    FROM StreakIdentifier\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    COUNT(DISTINCT product_id) AS products_with_3_consecutive_months\n",
    "FROM (\n",
    "    SELECT\n",
    "        product_id,\n",
    "        streak_group\n",
    "    FROM StreakGroups\n",
    "    GROUP BY product_id, streak_group\n",
    "    HAVING COUNT(purchase_month) >= 3\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "result1 = con.execute(query1).df()\n",
    "print(\"Result for Query 1:\")\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBQfOighHLtE"
   },
   "outputs": [],
   "source": [
    "query2 = f\"\"\"\n",
    "WITH DenormalizedGMV AS (\n",
    "    -- Assumption: mean = 150, std = 75.\n",
    "    SELECT\n",
    "        user_buyer_country,\n",
    "        purchase_payment_method,\n",
    "        (purchase_gmv * 75 + 150) AS denormalized_gmv\n",
    "\n",
    "    FROM '{csv_path}'\n",
    "\n",
    "    WHERE user_buyer_country IS NOT NULL AND purchase_payment_method IS NOT NULL\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    user_buyer_country,\n",
    "    purchase_payment_method AS source,\n",
    "    SUM(denormalized_gmv) AS gmv_for_combination,\n",
    "    (SUM(denormalized_gmv) * 100.0 / (SELECT SUM(denormalized_gmv) FROM DenormalizedGMV)) AS gmv_share_percentage\n",
    "\n",
    "FROM DenormalizedGMV\n",
    "\n",
    "GROUP BY user_buyer_country, source\n",
    "\n",
    "ORDER BY gmv_share_percentage DESC;\n",
    "\"\"\"\n",
    "\n",
    "result2 = con.execute(query2).df()\n",
    "print(\"\\nResult for Query 2:\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29QRfWduEp9_"
   },
   "source": [
    "## 1. DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb3Pp7rhEp-B"
   },
   "source": [
    "As we are dealing with a huge dataset, the only smart way to deal with it as a database, specifically a SQL database. But before creating our database, let's clean some mess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-76iuJSNEp-C"
   },
   "outputs": [],
   "source": [
    "input_file_path = 'purchases.csv'\n",
    "output_file_path = 'purchases_cleaned.csv'\n",
    "\n",
    "print(f\"Filepath to input file: {input_file_path}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_file_path)\n",
    "\n",
    "    # 1. Rename columns:\n",
    "    df.rename(columns={\n",
    "        'purchase_release_datetime': 'purchase_datetime',\n",
    "        'user_buyer_country': 'buyer_country',\n",
    "        'user_creator_country': 'creator_country',\n",
    "        'purchase_gmv': 'gmv_normalized'\n",
    "    }, inplace=True)\n",
    "    print(\"Colunas renomeadas.\")\n",
    "\n",
    "    # 2. Convert purchase_datetime to datetime format:\n",
    "    df['purchase_datetime'] = pd.to_datetime(df['purchase_datetime'], errors='coerce')\n",
    "\n",
    "    # 3. Denormalize the GMV:\n",
    "    # - Average: $150.00\n",
    "    # - Std. deviation: $75.00\n",
    "    mean_gmv = 150.0\n",
    "    std_gmv = 75.0\n",
    "\n",
    "    df['gmv_denormalized'] = (df['gmv_normalized'] * std_gmv) + mean_gmv\n",
    "\n",
    "    # Remove negative GMV values.\n",
    "    df['gmv_denormalized'] = df['gmv_denormalized'].apply(lambda x: max(0, x))\n",
    "\n",
    "    # 4. Save the dataframe to a new .csv file.\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Cleaned file saved to: '{output_file_path}'\")\n",
    "\n",
    "    print(\"\\nFinal dataframe summary:\")\n",
    "    df.info()\n",
    "\n",
    "    print(\"\\Dataframe sample:\")\n",
    "    print(df[['purchase_datetime', 'buyer_country', 'creator_country', 'gmv_normalized', 'gmv_denormalized']].head())\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: The file '{input_file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJjp_zZ3Ep-C"
   },
   "source": [
    "Looking at the data, we have some items to look at.\n",
    "\n",
    "To determine if we have a recurring purchase, we look at the `recurrency_type` column: if it's 'Single', it's a single purchase; if it's 'Recurring', it's a recurring purchase.\n",
    "\n",
    "But, a more keen look reveals that the consistency between `recurrency_type`, `purchase_parent_id`, and `purchase_recurrency_number` is flawed. To ensure everything goes well, let's diagnose the data.\n",
    "\n",
    "1.  **Orphaned recurrences:** a significant number of transactions marked as 'Recurring' do not have a `purchase_parent_id`, losing the link to the original purchase.\n",
    "\n",
    "2.  **Mislabeled child records:** transactions that have a `purchase_parent_id` (and are \"child\" records) are incorrectly marked as 'Single' or do not have a defined recurrence type (Null).\n",
    "\n",
    "3.  **Incomplete data:** a considerable volume of purchases that are structurally unique (don't have a `parent_id`) has a null `recurrency_type` field, representing a gap in the data entry.\n",
    "\n",
    "To avoid the chit-chat, let's use DuckDB to run a SQL query directly in the .csv file to know what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRz_UAHQEp-C"
   },
   "outputs": [],
   "source": [
    "csv_file_path = 'purchases_cleaned.csv'\n",
    "con = duckdb.connect()\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "CASE\n",
    "    WHEN purchase_parent_id IS NULL THEN 'None'\n",
    "    ELSE 'Existing'\n",
    "END AS purchase_parent,\n",
    "\n",
    "CASE\n",
    "    WHEN purchase_recurrency_number > 1 THEN 'Recurrent'\n",
    "    ELSE 'Single'\n",
    "END AS recurrency_type,\n",
    "\n",
    "CASE\n",
    "    WHEN purchase_recurrency_type = 'Single' THEN purchase_recurrency_type\n",
    "    ELSE purchase_recurrency_type\n",
    "END AS purchase_recurrency,\n",
    "\n",
    "COUNT(*) AS total\n",
    "\n",
    "FROM '{csv_file_path}'\n",
    "\n",
    "GROUP BY\n",
    "    purchase_parent,\n",
    "    recurrency_type,\n",
    "    purchase_recurrency\n",
    "\"\"\"\n",
    "\n",
    "result_df = con.execute(query).df()\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC3vUdhDEp-D"
   },
   "source": [
    "Here we meet some inconsistencies. We cannot have the absence of parent purchase and a purchase recurrency number, per example. What are we going to do now?\n",
    "\n",
    "* **Link reconstruction:** for the orphaned recurrences, the script will find the original \"parent\" transaction based on the purchase history of the same user for the same product, filling in the missing `purchase_parent_id`.\n",
    "\n",
    "* **Type correction:** for \"child\" transactions with an incorrect or NULL type, the `recurrency_type` will be duly adjusted to 'Recurring', reflecting their true nature.\n",
    "\n",
    "* **Filling gaps:** for unique purchases with a NULL `recurrency_type`, the field will be populated with 'Single', completing the missing information based on the transaction's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hk1Dhz0SEp-D"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. INITIAL SETUP ---\n",
    "print(\"Loading and preparing data...\")\n",
    "df = pd.read_csv('purchases_cleaned.csv')\n",
    "df['purchase_datetime'] = pd.to_datetime(df['purchase_datetime'])\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "\n",
    "# --- 2. NULLIFY RECURRENCE NUMBER '1' (As Requested) ---\n",
    "# Esta nova etapa remove o número de recorrência '1', tratando-os como compras únicas.\n",
    "print(\"\\n--- Applying New Rule: Nullifying Recurrence Number 1 ---\")\n",
    "condition_is_one = df['purchase_recurrency_number'] == 1\n",
    "count_ones = condition_is_one.sum()\n",
    "if count_ones > 0:\n",
    "    df.loc[condition_is_one, 'purchase_recurrency_number'] = np.nan\n",
    "    print(f\"FIXED: {count_ones} records with recurrence number 1 have been set to null.\")\n",
    "\n",
    "\n",
    "# --- 3. FIXING INCONSISTENT RECURRENCE TYPES ---\n",
    "# A lógica restante agora operará sobre os dados já ajustados.\n",
    "print(\"\\n--- Type Correction Step ---\")\n",
    "\n",
    "# Esta condição agora encontrará menos (ou zero) registros, o que é o esperado.\n",
    "cond_mislabeled_single = (df['purchase_recurrency_type'] == 'Single') & (df['purchase_recurrency_number'].notnull())\n",
    "if cond_mislabeled_single.sum() > 0:\n",
    "    df.loc[cond_mislabeled_single, 'purchase_recurrency_type'] = 'Recurring'\n",
    "    print(f\"FIXED: {cond_mislabeled_single.sum()} records that were 'Single' have been correctly changed to 'Recurring'.\")\n",
    "else:\n",
    "    print(\"INFO: No 'Single' records with a recurrence number were found to fix.\")\n",
    "\n",
    "# Preenche os tipos nulos com base na presença do parent_id.\n",
    "cond_fill_single = (df['purchase_parent_id'].isnull()) & (df['purchase_recurrency_type'].isnull())\n",
    "df.loc[cond_fill_single, 'purchase_recurrency_type'] = 'Single'\n",
    "cond_fill_recurring = (df['purchase_parent_id'].notnull()) & (df['purchase_recurrency_type'].isnull())\n",
    "df.loc[cond_fill_recurring, 'purchase_recurrency_type'] = 'Recurring'\n",
    "print(\"FIXED: Null recurrence types have been filled.\")\n",
    "\n",
    "\n",
    "# --- 4. RECONSTRUCTING PARENT_ID LINKS ---\n",
    "print(\"\\n--- Link Reconstruction Step ---\")\n",
    "df.sort_values(by=['buyer_id', 'product_id', 'purchase_datetime'], inplace=True)\n",
    "df['true_parent_id'] = df.groupby(['buyer_id', 'product_id'])['purchase_id'].transform('first')\n",
    "cond_orphan_recurrence = (df['purchase_parent_id'].isnull()) & (df['purchase_recurrency_type'] == 'Recurring') & (df['purchase_id'] != df['true_parent_id'])\n",
    "if cond_orphan_recurrence.sum() > 0:\n",
    "    df.loc[cond_orphan_recurrence, 'purchase_parent_id'] = df['true_parent_id']\n",
    "    print(f\"FIXED: {cond_orphan_recurrence.sum()} 'parent_id' links were reconstructed.\")\n",
    "df.drop(columns=['true_parent_id'], inplace=True)\n",
    "\n",
    "\n",
    "# --- 5. FILLING IN MISSING RECURRENCE NUMBERS ---\n",
    "print(\"\\n--- Number Filling Step ---\")\n",
    "df.sort_values(by=['purchase_parent_id', 'purchase_datetime'], inplace=True)\n",
    "cond_missing_number = (df['purchase_recurrency_type'] == 'Recurring') & (df['purchase_recurrency_number'].isnull())\n",
    "if cond_missing_number.sum() > 0:\n",
    "    valid_parents_df = df[df['purchase_parent_id'].notnull()]\n",
    "    sequential_count = valid_parents_df.groupby('purchase_parent_id').cumcount() + 1\n",
    "    df.loc[cond_missing_number, 'purchase_recurrency_number'] = sequential_count\n",
    "    print(f\"FIXED: {cond_missing_number.sum()} recurrence numbers have been filled.\")\n",
    "\n",
    "\n",
    "# --- 6. SAVING THE FINAL RESULT ---\n",
    "output_filename = 'purchases_cleaned.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nData treatment finished. The clean file has been saved as '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huCPxRbhEp-E"
   },
   "source": [
    "## 2. DATA AGGREGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3wBYigcEp-E"
   },
   "source": [
    "Now we are going to transform our large, cleaned transactional file (`purchases_cleaned.csv`, with ~1.8 million rows) into a single, lightweight summary file (`sellers_summary.csv`). This summary file will contain the key business indicators (KPIs) for **sellers**, already pre-calculated across the dimensions required for the executive dashboard (daily, by country, and by source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYnc0N8-Ep-E"
   },
   "outputs": [],
   "source": [
    "input_csv_path = 'purchases_cleaned.csv'\n",
    "summary_output_path = 'sellers_summary.csv'\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "print(f\"Reading file '{input_csv_path}' and starting aggregation...\")\n",
    "\n",
    "# SQL query to aggregate the data by day, country, and source\n",
    "query = f\"\"\"\n",
    "CREATE TABLE daily_summary AS\n",
    "SELECT\n",
    "    DATE_TRUNC('day', CAST(purchase_datetime AS TIMESTAMP)) AS purchase_date,\n",
    "    buyer_country,\n",
    "    purchase_payment_method AS source,\n",
    "    SUM(gmv_denormalized) AS total_gmv,\n",
    "    COUNT(DISTINCT buyer_id) AS unique_buyers,\n",
    "    COUNT(purchase_id) AS total_purchases\n",
    "FROM read_csv_auto('{input_csv_path}')\n",
    "GROUP BY 1, 2, 3;\n",
    "\"\"\"\n",
    "\n",
    "con.execute(query)\n",
    "\n",
    "print(f\"Aggregation complete. Exporting the summary to '{summary_output_path}'...\")\n",
    "con.execute(f\"COPY daily_summary TO '{summary_output_path}' (HEADER, DELIMITER ',');\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"Process finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTZD4iCBEp-F"
   },
   "source": [
    "This second file will be focused in the **customers**, focusing in their specific KPIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTZ9uf9XEp-F"
   },
   "outputs": [],
   "source": [
    "# Path to your main cleaned CSV file\n",
    "input_csv_path = 'purchases_cleaned.csv'\n",
    "# Name of the enriched customer summary file\n",
    "output_customer_summary_path = 'customer_summary.csv'\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "print(f\"Reading file '{input_csv_path}' to create the enriched customer summary...\")\n",
    "\n",
    "# SQL query modified to include installment metrics\n",
    "query = f\"\"\"\n",
    "CREATE TABLE customer_summary AS\n",
    "SELECT\n",
    "    buyer_id,\n",
    "    ANY_VALUE(buyer_country) as country,\n",
    "    MIN(CAST(purchase_datetime AS TIMESTAMP)) AS first_purchase_date,\n",
    "    MAX(CAST(purchase_datetime AS TIMESTAMP)) AS last_purchase_date,\n",
    "\n",
    "    -- Original KPIs\n",
    "    COUNT(purchase_id) AS lifetime_purchases,\n",
    "    SUM(gmv_denormalized) AS lifetime_gmv,\n",
    "    AVG(gmv_denormalized) AS average_purchase_value,\n",
    "\n",
    "    -- >>> NEW INSTALLMENT METRICS <<<\n",
    "    -- Counts how many purchases had more than 1 installment\n",
    "    COUNT(CASE WHEN purchase_installment_number > 1 THEN purchase_id END) AS installment_purchases_count,\n",
    "\n",
    "    -- Sums the GMV only from purchases with more than 1 installment\n",
    "    SUM(CASE WHEN purchase_installment_number > 1 THEN gmv_denormalized ELSE 0 END) AS lifetime_gmv_installments,\n",
    "\n",
    "    -- Calculates the average number of installments chosen, only for installment purchases\n",
    "    AVG(CASE WHEN purchase_installment_number > 1 THEN purchase_installment_number END) AS avg_installments_chosen\n",
    "\n",
    "FROM read_csv_auto('{input_csv_path}')\n",
    "WHERE buyer_id IS NOT NULL\n",
    "GROUP BY buyer_id;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the aggregation\n",
    "con.execute(query)\n",
    "\n",
    "# Export the aggregated result to the new CSV\n",
    "print(f\"Enriched customer aggregation complete. Exporting to '{output_customer_summary_path}'...\")\n",
    "con.execute(f\"COPY customer_summary TO '{output_customer_summary_path}' (HEADER, DELIMITER ',');\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(f\"Process finished! The file '{output_customer_summary_path}' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNRLRfu3Ep-F"
   },
   "source": [
    "The main trade-off of this highly efficient approach is the loss of **drill-down capability**. Because the dashboard will only contain this summarized data, it won't be possible to click on a chart (e.g., a specific day's sales) to see the underlying list of individual transactions. For the context of a high-level executive dashboard, this is often an acceptable trade-off to achieve performance and deliver key insights quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSYxZTnpEp-F"
   },
   "source": [
    "## 3. BONUS: DATA MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5Z_A-muEp-F"
   },
   "source": [
    "In the previous steps, we successfully cleaned and standardized our raw data, resulting in a single, large .csv file. But a single large table suffers from two main problems:\n",
    "\n",
    "1.  **Data redundancy:** information is repeated unnecessarily. For example, the details of a single product (like its `product_niche`) are stored again in every single row for every purchase of that product.\n",
    "\n",
    "2.  **Performance issues:** BI tools are slower when they have to scan and aggregate millions of rows from a wide, text-heavy table.\n",
    "\n",
    "To solve this, we will now transform our single flat file into a **Star Schema** divided into: a central **fact table** containing the quantitative measurements of our business (the purchases) and several surrounding **dimension tables**, each describing a specific business entity (who, what, where, when).\n",
    "\n",
    "To do it so, we are going to use the `duckdb` to execute SQL queries directly on our cleaned data. It will create and then export each table of our Star Schema into its own separate CSV file.\n",
    "\n",
    "1.  **Dimension Tables (`dim_`):**\n",
    "    * **Purpose:** To create small, clean, and fast lookup tables for the descriptive attributes of our data.\n",
    "    * **Process:** For each dimension (Users, Creators, Products, Sources), the script uses `SELECT DISTINCT` to get a unique list of entities, eliminating all redundancy.\n",
    "    * **Surrogate Keys:** For each dimension table, we generate a `_key` column (e.g., `user_key`) using `ROW_NUMBER()`. This creates a simple, unique integer ID for each record. Using these small integer keys for joining tables in a database is significantly faster than using the original long, text-based IDs.\n",
    "    * **`dim_date` (Special Case):** A proper date dimension is created separately using Pandas. This is crucial because it guarantees a complete calendar with no missing days, which is essential for accurate time-based analysis (e.g., comparing sales on days with no activity).\n",
    "\n",
    "2.  **Fact Table (`fct_purchases`):**\n",
    "    * **Purpose:** This is the core of our model, containing the transactional events.\n",
    "    * **Grain:** Each row represents a single purchase.\n",
    "    * **Process:** The script joins the original `purchases` table with the newly created dimension tables. The goal of these `LEFT JOIN` operations is to look up the integer `_key` from each dimension and place it in the fact table.\n",
    "    * **Result:** The final `fct_purchases` table is mostly composed of numbers: the business metrics (`gmv_denormalized`, etc.) and the foreign keys (`user_key`, `product_key`, etc.) that link to our dimension tables.\n",
    "\n",
    "3.  **Export to CSV:**\n",
    "    * Finally, the script saves each of the newly created tables (`dim_users`, `fct_purchases`, etc.) into its own dedicated `.csv` file. This set of interconnected files represents our final, analytics-ready data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubY03D0HEp-F"
   },
   "outputs": [],
   "source": [
    "# --- Etapa 1: Definição e Validação dos Arquivos ---\n",
    "# Certifique-se de que este é o nome correto do seu arquivo CSV final\n",
    "input_csv_path = 'purchases_cleaned.csv'\n",
    "\n",
    "if not os.path.exists(input_csv_path):\n",
    "    print(f\"--- ERRO CRÍTICO ---\")\n",
    "    print(f\"O arquivo de entrada '{input_csv_path}' não foi encontrado.\")\n",
    "    print(\"Por favor, verifique se o nome do arquivo está correto e se ele está na mesma pasta que o seu notebook.\")\n",
    "else:\n",
    "    # --- Etapa 2: Carregar Dados e Preparar Conexão ---\n",
    "    print(\"Iniciando o processo de modelagem de dados...\")\n",
    "\n",
    "    df_purchases = pd.read_csv(input_csv_path)\n",
    "    df_purchases['purchase_datetime'] = pd.to_datetime(df_purchases['purchase_datetime'])\n",
    "    print(f\"Arquivo '{input_csv_path}' carregado com sucesso.\")\n",
    "\n",
    "    con = duckdb.connect()\n",
    "    con.register('purchases', df_purchases)\n",
    "\n",
    "    # --- Etapa 3: Criação das Tabelas de Dimensão (Lógica Corrigida) ---\n",
    "    print(\"\\n--- Criando Tabelas de Dimensão ---\")\n",
    "\n",
    "    # dim_users\n",
    "    con.execute(\"\"\"\n",
    "    CREATE TABLE dim_users AS\n",
    "    SELECT\n",
    "        ROW_NUMBER() OVER() AS user_key,\n",
    "        buyer_id,\n",
    "        buyer_country\n",
    "    FROM (\n",
    "        SELECT DISTINCT buyer_id, buyer_country FROM purchases WHERE buyer_id IS NOT NULL\n",
    "    );\n",
    "    \"\"\")\n",
    "    print(\"Tabela 'dim_users' criada.\")\n",
    "\n",
    "    # dim_creators\n",
    "    con.execute(\"\"\"\n",
    "    CREATE TABLE dim_creators AS\n",
    "    SELECT\n",
    "        ROW_NUMBER() OVER() AS creator_key,\n",
    "        creator_id,\n",
    "        creator_country\n",
    "    FROM (\n",
    "        SELECT DISTINCT creator_id, creator_country FROM purchases WHERE creator_id IS NOT NULL\n",
    "    );\n",
    "    \"\"\")\n",
    "    print(\"Tabela 'dim_creators' criada.\")\n",
    "\n",
    "    # dim_products\n",
    "    con.execute(\"\"\"\n",
    "    CREATE TABLE dim_products AS\n",
    "    SELECT\n",
    "        ROW_NUMBER() OVER() AS product_key,\n",
    "        product_id,\n",
    "        product_format,\n",
    "        product_niche\n",
    "    FROM (\n",
    "        SELECT DISTINCT product_id, product_format, product_niche FROM purchases WHERE product_id IS NOT NULL\n",
    "    );\n",
    "    \"\"\")\n",
    "    print(\"Tabela 'dim_products' criada.\")\n",
    "\n",
    "    # dim_sources\n",
    "    con.execute(\"\"\"\n",
    "    CREATE TABLE dim_sources AS\n",
    "    SELECT\n",
    "        ROW_NUMBER() OVER() AS source_key,\n",
    "        purchase_payment_method\n",
    "    FROM (\n",
    "        SELECT DISTINCT purchase_payment_method FROM purchases WHERE purchase_payment_method IS NOT NULL\n",
    "    );\n",
    "    \"\"\")\n",
    "    print(\"Tabela 'dim_sources' criada.\")\n",
    "\n",
    "    # dim_date\n",
    "    min_date, max_date = df_purchases['purchase_datetime'].min(), df_purchases['purchase_datetime'].max()\n",
    "    date_range = pd.date_range(min_date.date(), max_date.date(), freq='D')\n",
    "    df_dates = pd.DataFrame(date_range, columns=['full_date'])\n",
    "    df_dates['date_key'] = df_dates['full_date'].dt.strftime('%Y%m%d').astype(int)\n",
    "    df_dates['year'] = df_dates['full_date'].dt.year\n",
    "    df_dates['month'] = df_dates['full_date'].dt.month\n",
    "    df_dates['day'] = df_dates['full_date'].dt.day\n",
    "    df_dates['day_of_week_name'] = df_dates['full_date'].dt.day_name()\n",
    "    df_dates['quarter'] = df_dates['full_date'].dt.quarter\n",
    "    con.register('dim_date_df', df_dates)\n",
    "    con.execute(\"CREATE TABLE dim_date AS SELECT * FROM dim_date_df;\")\n",
    "    print(\"Tabela 'dim_date' criada.\")\n",
    "\n",
    "    # --- Etapa 4: Criação da Tabela Fato ---\n",
    "    print(\"\\n--- Criando Tabela Fato ---\")\n",
    "    con.execute(\"\"\"\n",
    "    CREATE TABLE fct_purchases AS\n",
    "    SELECT\n",
    "        p.purchase_id, p.purchase_parent_id,\n",
    "        u.user_key, c.creator_key, pr.product_key, s.source_key,\n",
    "        CAST(strftime(CAST(p.purchase_datetime AS DATE), '%Y%m%d') AS INTEGER) AS date_key,\n",
    "        p.gmv_denormalized, p.purchase_installment_number, p.purchase_recurrency_number,\n",
    "        p.purchase_has_coupon, p.purchase_commission_affiliate, p.purchase_commission_cocreator\n",
    "    FROM\n",
    "        purchases p\n",
    "    LEFT JOIN dim_users u ON p.buyer_id = u.buyer_id\n",
    "    LEFT JOIN dim_creators c ON p.creator_id = c.creator_id\n",
    "    LEFT JOIN dim_products pr ON p.product_id = pr.product_id\n",
    "    LEFT JOIN dim_sources s ON p.purchase_payment_method = s.purchase_payment_method;\n",
    "    \"\"\")\n",
    "    print(\"Tabela 'fct_purchases' criada.\")\n",
    "\n",
    "    # --- Etapa 5: Salvar Todas as Tabelas ---\n",
    "    print(\"\\n--- Salvando Arquivos Finais ---\")\n",
    "    tables_to_save = ['dim_users', 'dim_creators', 'dim_products', 'dim_sources', 'dim_date', 'fct_purchases']\n",
    "    for table in tables_to_save:\n",
    "        con.execute(f\"COPY {table} TO '{table}.csv' (HEADER, DELIMITER ',');\")\n",
    "        print(f\"Arquivo '{table}.csv' salvo com sucesso.\")\n",
    "\n",
    "    con.close()\n",
    "    print(\"\\nProcesso concluído!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
